{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:30.190719Z",
     "iopub.status.busy": "2020-10-14T23:15:30.189573Z",
     "iopub.status.idle": "2020-10-14T23:15:30.193865Z",
     "shell.execute_reply": "2020-10-14T23:15:30.192923Z"
    },
    "papermill": {
     "duration": 0.035983,
     "end_time": "2020-10-14T23:15:30.194030",
     "exception": false,
     "start_time": "2020-10-14T23:15:30.158047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TPU = 'grpc://10.209.42.186:8470'\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_COUNT = 1790594\n",
    "IMAGE_SIZE = 256\n",
    "IMAGE_TRAIN_LEN = 56\n",
    "\n",
    "LOSS_WEIGHTS = {\n",
    "    'pe_present_on_image' : 0.0736196319 * 10, \n",
    "    'negative_exam_for_pe' : 0.0736196319,\n",
    "    'indeterminate' : 0.09202453988,\n",
    "    'chronic_pe' : 0.1042944785,\n",
    "    'acute_and_chronic_pe' : 0.1042944785,\n",
    "    'central_pe' : 0.1877300613,\n",
    "    'leftsided_pe' : 0.06257668712 ,\n",
    "    'rightsided_pe' : 0.06257668712,\n",
    "    'rv_lv_ratio_gte_1' : 0.2346625767,\n",
    "    'rv_lv_ratio_lt_1' : 0.0782208589,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:30.247944Z",
     "iopub.status.busy": "2020-10-14T23:15:30.247152Z",
     "iopub.status.idle": "2020-10-14T23:15:36.999882Z",
     "shell.execute_reply": "2020-10-14T23:15:36.999091Z"
    },
    "papermill": {
     "duration": 6.783396,
     "end_time": "2020-10-14T23:15:37.000029",
     "exception": false,
     "start_time": "2020-10-14T23:15:30.216633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'efficientnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-85b4a4cbe770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mefficientnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mefn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'efficientnet'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.sys.path.append('../input/rsna-sped-ds-utility/')\n",
    "import glob\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import efficientnet.tfkeras as efn \n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import layers, backend, models\n",
    "# from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:37.056398Z",
     "iopub.status.busy": "2020-10-14T23:15:37.055621Z",
     "iopub.status.idle": "2020-10-14T23:15:37.069508Z",
     "shell.execute_reply": "2020-10-14T23:15:37.068842Z"
    },
    "papermill": {
     "duration": 0.045011,
     "end_time": "2020-10-14T23:15:37.069663",
     "exception": false,
     "start_time": "2020-10-14T23:15:37.024652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:37.121007Z",
     "iopub.status.busy": "2020-10-14T23:15:37.119959Z",
     "iopub.status.idle": "2020-10-14T23:15:37.123381Z",
     "shell.execute_reply": "2020-10-14T23:15:37.122805Z"
    },
    "papermill": {
     "duration": 0.030839,
     "end_time": "2020-10-14T23:15:37.123524",
     "exception": false,
     "start_time": "2020-10-14T23:15:37.092685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "# #     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU,\n",
    "#                                                            zone='europe-west4-a',\n",
    "#                                                             project='dfdc-264702')\n",
    "#     print('Running on TPU ', tpu.master())\n",
    "# except ValueError:\n",
    "#     tpu = None\n",
    "\n",
    "# if tpu:\n",
    "#     tf.config.experimental_connect_to_cluster(tpu)\n",
    "#     tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "# else:\n",
    "#     strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "# print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:37.177040Z",
     "iopub.status.busy": "2020-10-14T23:15:37.176142Z",
     "iopub.status.idle": "2020-10-14T23:15:41.215223Z",
     "shell.execute_reply": "2020-10-14T23:15:41.214391Z"
    },
    "papermill": {
     "duration": 4.069031,
     "end_time": "2020-10-14T23:15:41.215358",
     "exception": false,
     "start_time": "2020-10-14T23:15:37.146327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/rsna-str-pulmonary-embolism-detection/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.267371Z",
     "iopub.status.busy": "2020-10-14T23:15:41.266497Z",
     "iopub.status.idle": "2020-10-14T23:15:41.270435Z",
     "shell.execute_reply": "2020-10-14T23:15:41.269678Z"
    },
    "papermill": {
     "duration": 0.032344,
     "end_time": "2020-10-14T23:15:41.270573",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.238229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = train_df.columns.values\n",
    "study_cols = list(cols[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.323325Z",
     "iopub.status.busy": "2020-10-14T23:15:41.322226Z",
     "iopub.status.idle": "2020-10-14T23:15:41.325803Z",
     "shell.execute_reply": "2020-10-14T23:15:41.325149Z"
    },
    "papermill": {
     "duration": 0.03137,
     "end_time": "2020-10-14T23:15:41.325938",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.294568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GCS_DS_PATH_TRAIN = KaggleDatasets().get_gcs_path('rsna-sped-ds-1011')\n",
    "GCS_DS_PATH_TRAIN = 'gs://rsna-sped/data_256_w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.381176Z",
     "iopub.status.busy": "2020-10-14T23:15:41.380101Z",
     "iopub.status.idle": "2020-10-14T23:15:41.383724Z",
     "shell.execute_reply": "2020-10-14T23:15:41.383086Z"
    },
    "papermill": {
     "duration": 0.034533,
     "end_time": "2020-10-14T23:15:41.383881",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.349348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fs = glob.glob('../input/rsna-sped-ds-1011/*.tfrec')\n",
    "fs = glob.glob('../data_256_w/*.tfrec')\n",
    "fs = sorted(fs)\n",
    "fs = [GCS_DS_PATH_TRAIN + '/' + os.path.basename(f) for f in fs]\n",
    "fs = np.array(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.457850Z",
     "iopub.status.busy": "2020-10-14T23:15:41.456286Z",
     "iopub.status.idle": "2020-10-14T23:15:41.464156Z",
     "shell.execute_reply": "2020-10-14T23:15:41.463390Z"
    },
    "papermill": {
     "duration": 0.056598,
     "end_time": "2020-10-14T23:15:41.464285",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.407687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4b352af0cad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# kf = KFold(n_splits=5, random_state=23, shuffle=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# kf = KFold(n_splits=5, random_state=23, shuffle=True)\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, valid_index in kf.split(fs):\n",
    "    break\n",
    "valid_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.527312Z",
     "iopub.status.busy": "2020-10-14T23:15:41.522065Z",
     "iopub.status.idle": "2020-10-14T23:15:41.551177Z",
     "shell.execute_reply": "2020-10-14T23:15:41.551758Z"
    },
    "papermill": {
     "duration": 0.063447,
     "end_time": "2020-10-14T23:15:41.551934",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.488487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_tfrecord(data):\n",
    "    features = {\n",
    "        \"study_labels\": tf.io.FixedLenFeature([13], tf.int64), \n",
    "        \"image_labels\": tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True), \n",
    "        \"images\": tf.io.FixedLenFeature([], tf.string), \n",
    "    }\n",
    "\n",
    "    # decode the TFRecord\n",
    "    tf_record = tf.io.parse_single_example(data, features)\n",
    "    \n",
    "    study_labels = tf_record['study_labels']\n",
    "    study_labels = tf.cast(study_labels, tf.float32)\n",
    "    image_labels = tf_record['image_labels']\n",
    "    image_labels = tf.cast(image_labels, tf.float32)\n",
    "    \n",
    "    images_bytes = tf.io.parse_tensor(tf_record['images'], tf.string)\n",
    "    \n",
    "    start = tf.constant(0, tf.int64)\n",
    "    end = tf.cast(tf.shape(image_labels)[0], tf.int64)\n",
    "    \n",
    "    t = tf.where(image_labels>0)\n",
    "    def gt():\n",
    "        return backend.min(t), backend.max(t)\n",
    "    \n",
    "    def le():\n",
    "        s = tf.random.uniform(shape=[], minval=backend.max(t) - IMAGE_TRAIN_LEN, maxval=backend.min(t), dtype=tf.int64)\n",
    "        s = tf.cond(s < 0, lambda: tf.constant(0, tf.int64), lambda: s)\n",
    "        s = tf.cond(s + IMAGE_TRAIN_LEN < end, lambda: s, lambda: end - IMAGE_TRAIN_LEN)\n",
    "        e = s + IMAGE_TRAIN_LEN\n",
    "        return s, e\n",
    "    \n",
    "    def pe():\n",
    "        return tf.cond(backend.max(t) - backend.min(t) > IMAGE_TRAIN_LEN, gt, le)\n",
    "    \n",
    "    def no_pe():\n",
    "        return start, end\n",
    "    \n",
    "    start, end = tf.cond(tf.greater(backend.sum(image_labels), tf.constant(0, tf.float32)), pe, no_pe)\n",
    "    \n",
    "    image_labels = image_labels[start:end]\n",
    "    images_bytes = images_bytes[start:end]\n",
    "    \n",
    "#     if np.sum(t[0]) > 0:\n",
    "#         tt = tf.where(t[0]>0)\n",
    "#         print(len(t[0]),  backend.max(tt) - backend.min(tt))\n",
    "\n",
    "\n",
    "    l = tf.cast(tf.shape(image_labels)[0], tf.int64)\n",
    "    n = l // IMAGE_TRAIN_LEN\n",
    "    \n",
    "    s = tf.cond(tf.equal(l%IMAGE_TRAIN_LEN, tf.constant(0, tf.int64)), \n",
    "                lambda: tf.constant(0, tf.int64), \n",
    "                lambda: tf.constant(1, tf.int64))\n",
    "    \n",
    "#     s = tf.random.uniform([], maxval=l-IMAGE_TRAIN_LEN*n+1, dtype=tf.int64)\n",
    "    i = tf.range(s, l, delta=n, dtype=tf.int64)\n",
    "    image_labels = tf.gather(image_labels, i[:IMAGE_TRAIN_LEN])\n",
    "    images_bytes = tf.gather(images_bytes, i[:IMAGE_TRAIN_LEN])\n",
    "    \n",
    "    images = []\n",
    "    for b in tf.unstack(images_bytes, num=IMAGE_TRAIN_LEN):\n",
    "        image = tf.image.decode_jpeg(b)\n",
    "        image = tf.image.random_jpeg_quality(image, 30, 100)\n",
    "        images.append(image)\n",
    "    del images_bytes\n",
    "    images = tf.stack(images)\n",
    "    images = tf.cast(images, tf.float32) / 256\n",
    "    images = tf.reshape(images, (IMAGE_TRAIN_LEN, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    \n",
    "    return images, (image_labels, \n",
    "                    study_labels[study_cols.index('negative_exam_for_pe')],\n",
    "                    study_labels[study_cols.index('indeterminate')], \n",
    "                    study_labels[study_cols.index('chronic_pe')], \n",
    "                    study_labels[study_cols.index('acute_and_chronic_pe')], \n",
    "                    study_labels[study_cols.index('central_pe')], \n",
    "                    study_labels[study_cols.index('leftsided_pe')], \n",
    "                    study_labels[study_cols.index('rightsided_pe')], \n",
    "                    study_labels[study_cols.index('rv_lv_ratio_gte_1')], \n",
    "                    study_labels[study_cols.index('rv_lv_ratio_lt_1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.607461Z",
     "iopub.status.busy": "2020-10-14T23:15:41.606678Z",
     "iopub.status.idle": "2020-10-14T23:15:41.609524Z",
     "shell.execute_reply": "2020-10-14T23:15:41.610105Z"
    },
    "papermill": {
     "duration": 0.032738,
     "end_time": "2020-10-14T23:15:41.610292",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.577554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = tf.data.TFRecordDataset(fs)\n",
    "# dataset = dataset.map(read_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.667280Z",
     "iopub.status.busy": "2020-10-14T23:15:41.666430Z",
     "iopub.status.idle": "2020-10-14T23:15:41.669922Z",
     "shell.execute_reply": "2020-10-14T23:15:41.669126Z"
    },
    "papermill": {
     "duration": 0.033099,
     "end_time": "2020-10-14T23:15:41.670068",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.636969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for index, (images, (t)) in enumerate(dataset.as_numpy_iterator()):\n",
    "# #     if t[1] == 0 and np.sum(t[0]) == 0:\n",
    "# #         print(t[0])\n",
    "#     if np.sum(t[0]) > 0:\n",
    "#         tt = tf.where(t[0]>0)\n",
    "#         print(len(t[0]),  backend.max(tt) - backend.min(tt))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.745089Z",
     "iopub.status.busy": "2020-10-14T23:15:41.734367Z",
     "iopub.status.idle": "2020-10-14T23:15:41.758819Z",
     "shell.execute_reply": "2020-10-14T23:15:41.758000Z"
    },
    "papermill": {
     "duration": 0.06381,
     "end_time": "2020-10-14T23:15:41.758952",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.695142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96\n",
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear = math.pi * shear / 180.\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1 = tf.math.cos(rotation)\n",
    "    s1 = tf.math.sin(rotation)\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
    "        \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)\n",
    "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
    "    \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    return backend.dot(backend.dot(rotation_matrix, shear_matrix), backend.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "def transform(images, l):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = IMAGE_SIZE\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    rot = 10. * tf.random.normal([1],dtype='float32')\n",
    "    shr = 3. * tf.random.normal([1],dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32') / 20.\n",
    "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32') / 20.\n",
    "    h_shift = 8. * tf.random.normal([1],dtype='float32') \n",
    "    w_shift = 8. * tf.random.normal([1],dtype='float32') \n",
    "  \n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
    "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
    "    z = tf.ones([DIM*DIM],dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = backend.dot(m,tf.cast(idx,dtype='float32'))\n",
    "    idx2 = backend.cast(idx2,dtype='int32')\n",
    "    idx2 = backend.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
    "    idx3 = tf.transpose(idx3)\n",
    "    \n",
    "    images = tf.transpose(images, perm=[1, 2, 0, 3])\n",
    "    images = tf.gather_nd(images, idx3)\n",
    "    images = tf.transpose(images, perm=[1, 0, 2])\n",
    "    images = tf.reshape(images, [IMAGE_TRAIN_LEN, DIM, DIM, 3])\n",
    "              \n",
    "    return images, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.813865Z",
     "iopub.status.busy": "2020-10-14T23:15:41.813100Z",
     "iopub.status.idle": "2020-10-14T23:15:41.816234Z",
     "shell.execute_reply": "2020-10-14T23:15:41.815473Z"
    },
    "papermill": {
     "duration": 0.032294,
     "end_time": "2020-10-14T23:15:41.816357",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.784063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# images = tf.constant(np.zeros((56, 256, 256, 3)))\n",
    "\n",
    "# idx3 = tf.constant(np.zeros((65536, 2)), dtype=tf.int64)\n",
    "# tf.gather_nd(images, idx3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.873664Z",
     "iopub.status.busy": "2020-10-14T23:15:41.872585Z",
     "iopub.status.idle": "2020-10-14T23:15:41.875356Z",
     "shell.execute_reply": "2020-10-14T23:15:41.875954Z"
    },
    "papermill": {
     "duration": 0.033386,
     "end_time": "2020-10-14T23:15:41.876116",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.842730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = tf.data.TFRecordDataset(fs)\n",
    "# dataset = dataset.map(read_tfrecord)\n",
    "# dataset = dataset.apply(tf.data.experimental.ignore_errors())\n",
    "# for i, (images, labels) in enumerate(dataset):\n",
    "#     print(i)\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.932395Z",
     "iopub.status.busy": "2020-10-14T23:15:41.931642Z",
     "iopub.status.idle": "2020-10-14T23:15:41.934479Z",
     "shell.execute_reply": "2020-10-14T23:15:41.935092Z"
    },
    "papermill": {
     "duration": 0.033145,
     "end_time": "2020-10-14T23:15:41.935252",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.902107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:41.991197Z",
     "iopub.status.busy": "2020-10-14T23:15:41.990362Z",
     "iopub.status.idle": "2020-10-14T23:15:41.993792Z",
     "shell.execute_reply": "2020-10-14T23:15:41.993053Z"
    },
    "papermill": {
     "duration": 0.033211,
     "end_time": "2020-10-14T23:15:41.993924",
     "exception": false,
     "start_time": "2020-10-14T23:15:41.960713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in images:\n",
    "#     plt.imshow(i[:,:,0])\n",
    "#     plt.show()\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:42.078867Z",
     "iopub.status.busy": "2020-10-14T23:15:42.077884Z",
     "iopub.status.idle": "2020-10-14T23:15:42.083641Z",
     "shell.execute_reply": "2020-10-14T23:15:42.082977Z"
    },
    "papermill": {
     "duration": 0.063643,
     "end_time": "2020-10-14T23:15:42.083773",
     "exception": false,
     "start_time": "2020-10-14T23:15:42.020130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3eb0758f00b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_tfrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_index' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(fs[train_index])\n",
    "train_dataset = train_dataset.map(read_tfrecord)\n",
    "train_dataset = train_dataset.map(transform)\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.prefetch(AUTO)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "valid_dataset = tf.data.TFRecordDataset(fs[valid_index])\n",
    "valid_dataset = valid_dataset.map(read_tfrecord)\n",
    "valid_dataset = valid_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "valid_dataset = valid_dataset.prefetch(AUTO)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:42.161713Z",
     "iopub.status.busy": "2020-10-14T23:15:42.147232Z",
     "iopub.status.idle": "2020-10-14T23:15:42.167769Z",
     "shell.execute_reply": "2020-10-14T23:15:42.168385Z"
    },
    "papermill": {
     "duration": 0.058357,
     "end_time": "2020-10-14T23:15:42.168571",
     "exception": false,
     "start_time": "2020-10-14T23:15:42.110214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-79f094d7a989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#     if t[1] == 0 and np.sum(t[0]) == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#         print(t[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for index, (images, (t)) in enumerate(train_dataset.as_numpy_iterator()):\n",
    "#     if t[1] == 0 and np.sum(t[0]) == 0:\n",
    "#         print(t[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:42.253297Z",
     "iopub.status.busy": "2020-10-14T23:15:42.247063Z",
     "iopub.status.idle": "2020-10-14T23:15:42.257730Z",
     "shell.execute_reply": "2020-10-14T23:15:42.258286Z"
    },
    "papermill": {
     "duration": 0.059608,
     "end_time": "2020-10-14T23:15:42.258472",
     "exception": false,
     "start_time": "2020-10-14T23:15:42.198864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2d752c8e7720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "for image in images[0]:\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:42.345912Z",
     "iopub.status.busy": "2020-10-14T23:15:42.339110Z",
     "iopub.status.idle": "2020-10-14T23:15:42.349157Z",
     "shell.execute_reply": "2020-10-14T23:15:42.348352Z"
    },
    "papermill": {
     "duration": 0.062567,
     "end_time": "2020-10-14T23:15:42.349291",
     "exception": false,
     "start_time": "2020-10-14T23:15:42.286724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv3D(filters1, (1, 1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv3D(filters2, kernel_size,\n",
    "                      padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv3D(filters3, (1, 1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(1, 2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        strides: Strides for the first conv layer in the block.\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3,\n",
    "    the first conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv3D(filters1, (1, 1, 1), strides=strides,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv3D(filters2, kernel_size, padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv3D(filters3, (1, 1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = layers.Conv3D(filters3, (1, 1, 1), strides=strides,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = layers.BatchNormalization(\n",
    "        axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:42.442481Z",
     "iopub.status.busy": "2020-10-14T23:15:42.437198Z",
     "iopub.status.idle": "2020-10-14T23:15:42.631836Z",
     "shell.execute_reply": "2020-10-14T23:15:42.631065Z"
    },
    "papermill": {
     "duration": 0.254898,
     "end_time": "2020-10-14T23:15:42.631966",
     "exception": false,
     "start_time": "2020-10-14T23:15:42.377068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-97b5af0a832c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-97b5af0a832c>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroPadding3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1_pad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    def create_model():\n",
    "        n = 32\n",
    "        x_input = layers.Input(shape=(None, 256, 256, 3))\n",
    "        x = x_input\n",
    "        x = layers.ZeroPadding3D(padding=(0, 3, 3), name='conv1_pad')(x)\n",
    "        x = layers.Conv3D(64, (1, 7, 7),\n",
    "                              strides=(1, 2, 2),\n",
    "                              padding='valid',\n",
    "                              kernel_initializer='he_normal',\n",
    "                              name='conv1')(x)\n",
    "        x = layers.BatchNormalization(name='bn_conv1')(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.ZeroPadding3D(padding=(0, 1, 1), name='pool1_pad')(x)\n",
    "        x = layers.MaxPooling3D((1, 3, 3), strides=(1, 2, 2))(x)\n",
    "        \n",
    "        x = conv_block(x, (1, 3, 3), [n, n, n*4], stage=2, block='a', strides=(1, 1, 1))\n",
    "        x = identity_block(x, (1, 3, 3), [n, n, n*4], stage=2, block='b')\n",
    "        x = identity_block(x, (1, 3, 3), [n, n, n*4], stage=2, block='c')\n",
    "\n",
    "        x = conv_block(x, (1, 3, 3), [n*2, n*2, n*8], stage=3, block='a')\n",
    "        x = identity_block(x, (1, 3, 3), [n*2, n*2, n*8], stage=3, block='b')\n",
    "        x = identity_block(x, (1, 3, 3), [n*2, n*2, n*8], stage=3, block='c')\n",
    "        x = identity_block(x, (1, 3, 3), [n*2, n*2, n*8], stage=3, block='d')\n",
    "\n",
    "        x = conv_block(x, 3, [n*4, n*4, n*16], stage=4, block='a')\n",
    "        x = identity_block(x, 3, [n*4, n*4, n*16], stage=4, block='b')\n",
    "        x = identity_block(x, 3, [n*4, n*4, n*16], stage=4, block='c')\n",
    "        x = identity_block(x, 3, [n*4, n*4, n*16], stage=4, block='d')\n",
    "        x = identity_block(x, 3, [n*4, n*4, n*16], stage=4, block='e')\n",
    "        x = identity_block(x, 3, [n*4, n*4, n*16], stage=4, block='f')\n",
    "\n",
    "        x = conv_block(x, 3, [n*8, n*8, n*32], stage=5, block='a')\n",
    "        x = identity_block(x, 3, [n*8, n*8, n*32], stage=5, block='b')\n",
    "        x = identity_block(x, 3, [n*8, n*8, n*32], stage=5, block='c')\n",
    "        \n",
    "        x_i = backend.max(x, axis=[2, 3], keepdims=True)\n",
    "        x_i = tf.keras.layers.Dense(1, activation='sigmoid')(x_i)\n",
    "#         pe_present_on_image = tf.keras.layers.Reshape([IMAGE_TRAIN_LEN], name='pe_present_on_image')(x_i)\n",
    "        pe_present_on_image = tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=[2,3,4]), name='pe_present_on_image')(x_i)\n",
    "        \n",
    "        x_s = layers.GlobalMaxPool3D()(x)\n",
    "        \n",
    "        negative_exam_for_pe = tf.keras.layers.Dense(1, activation='sigmoid', name='negative_exam_for_pe')(x_s)\n",
    "        indeterminate = tf.keras.layers.Dense(1, activation='sigmoid', name='indeterminate')(x_s)\n",
    "        chronic_pe = tf.keras.layers.Dense(1, activation='sigmoid', name='chronic_pe')(x_s)\n",
    "        acute_and_chronic_pe = tf.keras.layers.Dense(1, activation='sigmoid', name='acute_and_chronic_pe')(x_s)\n",
    "        central_pe = tf.keras.layers.Dense(1, activation='sigmoid', name='central_pe')(x_s)\n",
    "        leftsided_pe = tf.keras.layers.Dense(1, activation='sigmoid', name='leftsided_pe')(x_s)\n",
    "        rightsided_pe = tf.keras.layers.Dense(1, activation='sigmoid', name='rightsided_pe')(x_s)\n",
    "        rv_lv_ratio_gte_1 = tf.keras.layers.Dense(1, activation='sigmoid', name='rv_lv_ratio_gte_1')(x_s)\n",
    "        rv_lv_ratio_lt_1 = tf.keras.layers.Dense(1, activation='sigmoid', name='rv_lv_ratio_lt_1')(x_s)\n",
    "        \n",
    "        model = tf.keras.models.Model(x_input, [pe_present_on_image, \n",
    "                                                negative_exam_for_pe,\n",
    "                                                indeterminate,\n",
    "                                                chronic_pe,\n",
    "                                                acute_and_chronic_pe,\n",
    "                                                central_pe,\n",
    "                                                leftsided_pe,\n",
    "                                                rightsided_pe,\n",
    "                                                rv_lv_ratio_gte_1,\n",
    "                                                rv_lv_ratio_lt_1\n",
    "                                               ])\n",
    "#         optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "        optimizer = tf.keras.optimizers.Adamax()\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        model.compile(optimizer, loss, loss_weights=LOSS_WEIGHTS)\n",
    "        return model \n",
    "\n",
    "    model = create_model()\n",
    "    model.summary(line_length=130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T23:15:42.720352Z",
     "iopub.status.busy": "2020-10-14T23:15:42.719253Z",
     "iopub.status.idle": "2020-10-14T23:15:42.724374Z",
     "shell.execute_reply": "2020-10-14T23:15:42.723500Z"
    },
    "papermill": {
     "duration": 0.063417,
     "end_time": "2020-10-14T23:15:42.724509",
     "exception": false,
     "start_time": "2020-10-14T23:15:42.661092",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c3b6b6353c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model.fit(train_dataset,\n\u001b[0m\u001b[1;32m      7\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7279\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "class ValidCB(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        model.save('resnet3d_{:03d}_{:.05f}.h5'.format(epoch, logs['val_loss']))\n",
    "        print('')\n",
    "        \n",
    "model.fit(train_dataset,\n",
    "          validation_data=valid_dataset,\n",
    "          steps_per_epoch=int(7279 * 0.8 / BATCH_SIZE),\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          callbacks=[ValidCB()],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.029098,
     "end_time": "2020-10-14T23:15:42.784662",
     "exception": false,
     "start_time": "2020-10-14T23:15:42.755564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.028555,
     "end_time": "2020-10-14T23:15:42.842193",
     "exception": false,
     "start_time": "2020-10-14T23:15:42.813638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.028739,
     "end_time": "2020-10-14T23:15:42.900442",
     "exception": false,
     "start_time": "2020-10-14T23:15:42.871703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 17.992096,
   "end_time": "2020-10-14T23:15:43.040190",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-14T23:15:25.048094",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
